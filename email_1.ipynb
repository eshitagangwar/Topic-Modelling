{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "email 1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fljixvde-T3v",
        "colab_type": "code",
        "outputId": "47f087d9-ace9-4cec-9967-fa14efe60075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "import sys\n",
        "import re, numpy as np, pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim, spacy, logging, warnings\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import lemmatize, simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NLTK Stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
        "\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "# Import Dataset\n",
        "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
        "print(df.shape) \n",
        "df['content'].head(10)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11314, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        From: lerxst@wam.umd.edu (where's my thing)\\nS...\n",
              "1        From: guykuo@carson.u.washington.edu (Guy Kuo)...\n",
              "10       From: irwin@cmptrc.lonestar.org (Irwin Arnstei...\n",
              "100      From: tchen@magnus.acs.ohio-state.edu (Tsung-K...\n",
              "1000     From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...\n",
              "10000    From: a207706@moe.dseg.ti.com (Robert Loper)\\n...\n",
              "10001    From: kimman@magnus.acs.ohio-state.edu (Kim Ri...\n",
              "10002    From: kwilson@casbah.acns.nwu.edu (Kirtley Wil...\n",
              "10003    Subject: Re: Don't more innocents die without ...\n",
              "10004    From: livesey@solntze.wpd.sgi.com (Jon Livesey...\n",
              "Name: content, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ndnsrh5uxoDP",
        "colab_type": "code",
        "outputId": "f1d9cf5c-1376-40ea-a6dc-6d59972ec0e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgTOk_Fn_gUT",
        "colab_type": "text"
      },
      "source": [
        "3. Tokenize Sentences and Clean\n",
        "Removing the emails, new line characters, single quotes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMnfvjIV_EZ0",
        "colab_type": "code",
        "outputId": "869a2c89-9ff0-4b0a-bdb1-b12a7baaaaaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "def sent_to_words(sentences):\n",
        "  print(sentences)\n",
        "  \n",
        "  for sent in sentences:\n",
        "        sent = re.sub('\\S*@\\S*\\s?', '', sent)\n",
        "        \n",
        "        sent = re.sub('\\s+', ' ', sent)\n",
        "        sent = re.sub(\"\\'\", \"\", sent)  \n",
        "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
        "        yield(sent)  \n",
        "\n",
        "\n",
        "data = df.content.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1])\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P2DFRlb_zFT",
        "colab_type": "text"
      },
      "source": [
        "4.  lemmatize each word to its root form, keeping only nouns, adjectives, verbs and adverbs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLEJSFKJ_zX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \n",
        "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "    \n",
        "    texts_out = []\n",
        "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    \n",
        "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
        "    return texts_out\n",
        "\n",
        "data_ready = process_words(data_words)  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHgrPlx9_W1B",
        "colab_type": "text"
      },
      "source": [
        "5. Build the Topic Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvv4fIqG_Eb5",
        "colab_type": "code",
        "outputId": "073c9323-8cff-4477-b0ac-4d699f4b80e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1344
        }
      },
      "source": [
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_ready)\n",
        "\n",
        "# Create Corpus: Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics= 20)\n",
        "\n",
        "pprint(lda_model.print_topics())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0,\n",
            "  '0.010*\"organization\" + 0.006*\"host\" + 0.006*\"post\" + 0.006*\"nntp\" + '\n",
            "  '0.006*\"disk\" + 0.006*\"university\" + 0.005*\"drive\" + 0.005*\"cec\" + '\n",
            "  '0.004*\"write\" + 0.004*\"boot\"'),\n",
            " (1,\n",
            "  '0.011*\"car\" + 0.009*\"organization\" + 0.009*\"article\" + 0.008*\"write\" + '\n",
            "  '0.006*\"time\" + 0.005*\"bike\" + 0.004*\"thing\" + 0.004*\"state\" + 0.004*\"new\" + '\n",
            "  '0.004*\"look\"'),\n",
            " (2,\n",
            "  '0.010*\"people\" + 0.008*\"god\" + 0.007*\"write\" + 0.005*\"organization\" + '\n",
            "  '0.005*\"believe\" + 0.005*\"question\" + 0.005*\"article\" + 0.004*\"mean\" + '\n",
            "  '0.004*\"give\" + 0.004*\"israeli\"'),\n",
            " (3,\n",
            "  '0.011*\"gordon\" + 0.010*\"bank\" + 0.008*\"organization\" + 0.008*\"article\" + '\n",
            "  '0.007*\"write\" + 0.006*\"science\" + 0.005*\"reply\" + 0.005*\"helmet\" + '\n",
            "  '0.004*\"gary\" + 0.004*\"dare\"'),\n",
            " (4,\n",
            "  '0.019*\"space\" + 0.005*\"organization\" + 0.005*\"post\" + 0.005*\"nasa\" + '\n",
            "  '0.005*\"scsi\" + 0.004*\"launch\" + 0.004*\"write\" + 0.004*\"mission\" + '\n",
            "  '0.004*\"orbit\" + 0.004*\"article\"'),\n",
            " (5,\n",
            "  '0.015*\"organization\" + 0.012*\"post\" + 0.010*\"university\" + 0.008*\"host\" + '\n",
            "  '0.008*\"nntp\" + 0.008*\"write\" + 0.008*\"article\" + 0.005*\"problem\" + '\n",
            "  '0.004*\"work\" + 0.004*\"anyone\"'),\n",
            " (6,\n",
            "  '0.008*\"key\" + 0.008*\"gun\" + 0.007*\"government\" + 0.006*\"chip\" + 0.006*\"law\" '\n",
            "  '+ 0.006*\"system\" + 0.005*\"encryption\" + 0.005*\"organization\" + '\n",
            "  '0.005*\"clipper\" + 0.005*\"people\"'),\n",
            " (7,\n",
            "  '0.011*\"write\" + 0.009*\"system\" + 0.009*\"organization\" + 0.008*\"controller\" '\n",
            "  '+ 0.008*\"scsi\" + 0.007*\"keith\" + 0.006*\"ide\" + 0.006*\"drive\" + 0.006*\"bus\" '\n",
            "  '+ 0.005*\"article\"'),\n",
            " (8,\n",
            "  '0.009*\"write\" + 0.007*\"christian\" + 0.007*\"people\" + 0.006*\"article\" + '\n",
            "  '0.005*\"god\" + 0.005*\"organization\" + 0.005*\"man\" + 0.004*\"believe\" + '\n",
            "  '0.004*\"time\" + 0.004*\"new\"'),\n",
            " (9,\n",
            "  '0.013*\"window\" + 0.009*\"organization\" + 0.009*\"file\" + 0.007*\"system\" + '\n",
            "  '0.007*\"card\" + 0.007*\"problem\" + 0.006*\"write\" + 0.006*\"drive\" + '\n",
            "  '0.005*\"work\" + 0.005*\"post\"'),\n",
            " (10,\n",
            "  '0.015*\"key\" + 0.006*\"organization\" + 0.006*\"write\" + 0.005*\"information\" + '\n",
            "  '0.004*\"block\" + 0.004*\"article\" + 0.004*\"post\" + 0.003*\"cipher\" + '\n",
            "  '0.003*\"pgp\" + 0.003*\"part\"'),\n",
            " (11,\n",
            "  '0.009*\"sale\" + 0.007*\"organization\" + 0.007*\"price\" + 0.007*\"include\" + '\n",
            "  '0.006*\"new\" + 0.006*\"offer\" + 0.004*\"post\" + 0.004*\"system\" + 0.004*\"host\" '\n",
            "  '+ 0.004*\"book\"'),\n",
            " (12,\n",
            "  '0.800*\"ax\" + 0.057*\"max\" + 0.004*\"tm\" + 0.004*\"bhj\" + 0.003*\"giz\" + '\n",
            "  '0.003*\"qax\" + 0.001*\"gq\" + 0.001*\"nrhj\" + 0.001*\"biz\" + 0.001*\"mg\"'),\n",
            " (13,\n",
            "  '0.014*\"write\" + 0.011*\"article\" + 0.009*\"organization\" + '\n",
            "  '0.007*\"stephanopoulo\" + 0.006*\"people\" + 0.005*\"post\" + 0.005*\"well\" + '\n",
            "  '0.004*\"mr\" + 0.004*\"university\" + 0.004*\"tax\"'),\n",
            " (14,\n",
            "  '0.011*\"window\" + 0.008*\"player\" + 0.008*\"game\" + 0.007*\"win\" + '\n",
            "  '0.007*\"organization\" + 0.006*\"write\" + 0.005*\"manager\" + 0.005*\"widget\" + '\n",
            "  '0.005*\"post\" + 0.004*\"host\"'),\n",
            " (15,\n",
            "  '0.009*\"organization\" + 0.006*\"file\" + 0.006*\"write\" + 0.006*\"post\" + '\n",
            "  '0.005*\"entry\" + 0.005*\"host\" + 0.004*\"article\" + 0.004*\"nntp\" + '\n",
            "  '0.004*\"program\" + 0.004*\"com\"'),\n",
            " (16,\n",
            "  '0.016*\"armenian\" + 0.008*\"people\" + 0.007*\"turkish\" + 0.006*\"president\" + '\n",
            "  '0.004*\"turk\" + 0.004*\"tell\" + 0.004*\"time\" + 0.004*\"armenia\" + 0.004*\"msg\" '\n",
            "  '+ 0.004*\"argic\"'),\n",
            " (17,\n",
            "  '0.016*\"team\" + 0.015*\"game\" + 0.011*\"year\" + 0.009*\"play\" + '\n",
            "  '0.009*\"organization\" + 0.008*\"hockey\" + 0.008*\"season\" + 0.007*\"player\" + '\n",
            "  '0.007*\"win\" + 0.006*\"league\"'),\n",
            " (18,\n",
            "  '0.010*\"organization\" + 0.007*\"write\" + 0.006*\"gif\" + 0.006*\"post\" + '\n",
            "  '0.006*\"article\" + 0.005*\"university\" + 0.004*\"nntp\" + 0.004*\"host\" + '\n",
            "  '0.004*\"system\" + 0.004*\"anyone\"'),\n",
            " (19,\n",
            "  '0.007*\"organization\" + 0.006*\"post\" + 0.005*\"host\" + 0.004*\"write\" + '\n",
            "  '0.004*\"nntp\" + 0.004*\"rider\" + 0.004*\"new\" + 0.004*\"cover\" + 0.004*\"linux\" '\n",
            "  '+ 0.004*\"risc\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4lP77Y-HMiv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# New Document to be classified"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrUZj7uEHMpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_doc = \"Looping correct email addresses of Shivali and Eshita! On Tue, 14 May 2019, 14:30 Nikita Bafna, <nikitabafna04@gmail.com> wrote: Hi Hannah,Hope you are well.Thank you for the updates.Could you please attach the map of that side of community, it seems you missed it.Also, can you clarify what is a downstairs unit? Regards,Nikita On Tue, May 14, 2019 at 12:48 AM Woodbridge Bloomington <WoodbridgeBloomington@glickco.com> wrote:Hi Nikita, Shivali, Neha and Eshita,I hope youre doing well! I wanted to let you know that weve begun looking over our apartments.Due to the high demand for two bedroom town home units, we only have one available option that fits your preferences.This apartment is located at 759 Woodbridge Drive, this is a downstairs unit end unit, and will be available to move into by July 31st.Ive included a map of that side of our community,  and will have to notify the residents of 759 that we have someone interested in their apartment, as well.Please let us know if this will work for you as soon as possible so that we can move forward with the next step.I look forward to hearing from you!\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFsG7mJbHCYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent_to_words_new(sentences):\n",
        "  print(sentences)\n",
        "  sent = re.sub('\\S*@\\S*\\s?', '', sentences) \n",
        "  sent = re.sub('\\s+', ' ', sent)\n",
        "  sent = re.sub(\"\\'\", \"\", sent)  \n",
        "  sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
        "  yield(sent)  \n",
        "\n",
        "def process_words_new(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \n",
        "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "    texts_out = []\n",
        "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    \n",
        "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
        "    return texts_out\n",
        "\n",
        "\n",
        "def running_lda(new_doc):\n",
        "  data_new_words = list(sent_to_words_new(new_doc))\n",
        "  data_ready_new = process_words_new(data_new_words) \n",
        "  \n",
        "  id2word_new = corpora.Dictionary(data_ready)\n",
        "  corpus_new = [id2word.doc2bow(text) for text in data_ready_new]\n",
        "  \n",
        "  doc2topic_prob = lda_model.get_document_topics(corpus_new)\n",
        " \n",
        "  topics = []\n",
        "  prob = []\n",
        "  for i in list(doc2topic_prob):\n",
        "    for j in i:\n",
        "      topics.append(j[0])\n",
        "      prob.append(j[1])\n",
        "  doc2topic= pd.DataFrame()\n",
        "  doc2topic['topics'] = pd.Series(topics)\n",
        "  doc2topic['prob'] = pd.Series(prob)\n",
        " \n",
        "  topic = doc2topic.sort_values('prob',ascending=False).head(1)\n",
        "  return(topic)\n",
        "\n",
        "  \n",
        "#running_lda(new_doc1)\n",
        "#print(running_lda(new_doc1), 'dominat topic')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUKMBE7TulOw",
        "colab_type": "text"
      },
      "source": [
        "Text summarization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJo1hi4vMTAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import regex as re \n",
        "def read_article(file_name):\n",
        "    article = file_name.split(\".\")\n",
        "    sentences = []\n",
        "\n",
        "    for sent in article:\n",
        "        sent = sent.lower()\n",
        "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
        "        #sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
        "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
        "        sent = re.sub(\"-\" , \" \", sent)\n",
        "        sentences.append(sent.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
        "    #sentences.pop() \n",
        "    \n",
        "    return sentences\n",
        "\n",
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        " \n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        " \n",
        "    all_words = list(set(sent1 + sent2))\n",
        " \n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        " \n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        " \n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        " \n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        " \n",
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        " \n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue \n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "\n",
        "def generate_summary(file_name, top_n=5):\n",
        "    stop_words = stopwords.words('english')\n",
        "    stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
        "\n",
        "    summarize_text = []\n",
        "\n",
        "    # Step 1 - Read text anc split it\n",
        "    sentences =  read_article(file_name)\n",
        "\n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
        "    #print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
        "\n",
        "    for i in range(top_n):\n",
        "        summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        "\n",
        "    # Step 5 - Offcourse, output the summarize texr\n",
        "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
        "\n",
        "# let's begin\n",
        "#generate_summary( new_doc, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5j8NupUMTDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wHRkiPxzN-m",
        "colab_type": "text"
      },
      "source": [
        "Example:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGa7ffEDIqJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_doc1 = \" From abc@def Hockey fever has gripped Bhubaneswar city and the nation at large as 16 hockey playing nations will lock horns for World Cup glory starting November 28. The fever has reached the nook and corner of the country through 'My Heart Beats for Hockey 'campaign'. Not just matches, the global event will witness a star-studded opening ceremony with Bollywood icons Shahrukh Khan, Salman Khan, Madhuri Dixit and music maestro AR Rahman set to rock the stage. The opening ceremoby will start at 5:30 pm on Tuesday evening. Elaborate security arrangement has been made to avoid untoward incidents. Tickets for the opening ceremony and India matches have been sold out. The anticipation and excitement is palpable among hockey aficionados in the country and across the globe and Odisha government is also leaving no stones unturned to use the opportunity to boost tourist footfall in the state. The 14th edition of the mega event will witness 36 matches between 28 November and 16 December with World No 1 Australia as defending champions. The opening match of the tournament will see World No 3 Belgium taking on World No 11 Canada on November 28, 2018, while host nation India will open their campaign on the same day against South Africa.No doubt, Indian national team will be the favourites among the formidable opponents in front of its home crowd. The state has spent Rs 820 million to give build new infrastructure and mount branding and publicity campaigns in the run-up to the event.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH3xHfeM0GtX",
        "colab_type": "code",
        "outputId": "8bf0dff5-353f-4fd4-ac28-1a9eb64e1a61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "generate_summary( new_doc1, 3)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summarize Text: \n",
            " .  tickets for the opening ceremony and india matches have been sold out.  the fever has reached the nook and corner of the country through my heart beats for hockey campaign\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI9bPnbV0Mhu",
        "colab_type": "code",
        "outputId": "82816b10-5004-4d8a-c236-f6037c933f47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        }
      },
      "source": [
        "running_lda(new_doc1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " From abc@def Hockey fever has gripped Bhubaneswar city and the nation at large as 16 hockey playing nations will lock horns for World Cup glory starting November 28. The fever has reached the nook and corner of the country through 'My Heart Beats for Hockey 'campaign'. Not just matches, the global event will witness a star-studded opening ceremony with Bollywood icons Shahrukh Khan, Salman Khan, Madhuri Dixit and music maestro AR Rahman set to rock the stage. The opening ceremoby will start at 5:30 pm on Tuesday evening. Elaborate security arrangement has been made to avoid untoward incidents. Tickets for the opening ceremony and India matches have been sold out. The anticipation and excitement is palpable among hockey aficionados in the country and across the globe and Odisha government is also leaving no stones unturned to use the opportunity to boost tourist footfall in the state. The 14th edition of the mega event will witness 36 matches between 28 November and 16 December with World No 1 Australia as defending champions. The opening match of the tournament will see World No 3 Belgium taking on World No 11 Canada on November 28, 2018, while host nation India will open their campaign on the same day against South Africa.No doubt, Indian national team will be the favourites among the formidable opponents in front of its home crowd. The state has spent Rs 820 million to give build new infrastructure and mount branding and publicity campaigns in the run-up to the event.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topics</th>\n",
              "      <th>prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>16</td>\n",
              "      <td>0.272225</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   topics      prob\n",
              "7      16  0.272225"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ8e7G0z1VTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}